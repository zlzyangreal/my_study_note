##  **batch-size**
批次大小，也就是一次训练选取的样本个数
* batch-size的大小对模型的优化和速度都是很有影响的．尤其是你GPU的个数不多时，最好不要把数值设置的很大．
* batchsize的正确选择是为了在内存效率和内存容量之间寻找最佳平衡
**适当的增加Batch_Size的优点**:
1. 通过并行化提高内存利用率
2. 单次epoch的迭代次数减少，提高运行速度。（单次epoch=(全部训练样本/batchsize)/iteration=1）
3. 适当的增加Batch_Size,梯度下降方向准确度增加，训练震动的幅度减小
**tips**:
1. 相对于正常数据集，如果Batch_Size过小，训练数据就会非常难收敛，从而导致underfitting
2. 增大Batch_Size,相对处理速度加快
3. 增大Batch_Size,所需内存容量增加（epoch的次数需要增加以达到最好的结果）
**Batch_Size的正确选择是为了在内存效率和内存容量之间寻找最佳平衡**
##  **epoch**
1个epoch指用训练集中的全部样本训练一次，此时相当于batchsize 等于训练集的样本数
epoch数是一个超参数，它定义了学习算法在整个训练数据集中的工作次数。一个Epoch意味着训练数据集中的每个样本都有机会更新内部模型参数。Epoch由一个或多个Batch组成, 具有一批的Epoch称为批量梯度下降学习算法。您可以将for循环放在每个需要遍历训练数据集的epoch上，在这个for循环中是另一个嵌套的for循环，它遍历每批样本，其中一个批次具有指定的“批量大小”样本数。
当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一次epoch。然而，当一个epoch对于计算机而言太庞大的时候，就需要把它分成多个小块。
**为什么要使用多于一个epoch?**
在神经网络中传递完整的数据集一次是不够的，而且我们需要将完整的数据集在同样的神经网络中传递多次。但请记住，我们使用的是有限的数据集，并且我们使用一个迭代过程即梯度下降来优化学习过程。因此仅仅更新一次或者说使用一个epoch是不够的。
随着epoch数量增加，神经网络中的权重的更新次数也在增加，曲线从欠拟合变得过拟合。
## **iteration**
1个iteration即迭代一次，也就是用batchsize个样本训练一次。
迭代是**重复反馈**的动作，神经网络中我们希望通过迭代进行多次的训练以到达所需的目标或结果