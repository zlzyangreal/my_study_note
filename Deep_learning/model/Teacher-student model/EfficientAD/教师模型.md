# 教师模型
```python
if __name__ == '__main__':
    imagenet_dir = './data/ImageNet'
    channel_size = 384
    save_path = './ckptSmall'
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    distillation_training = DistillationTraining(
        imagenet_dir,channel_size,16,save_path,
        normalize_iter=500, 
        model_size='S',
        iteration=10000,
        wide_resnet_101_arch="Wide_ResNet101_2_Weights.IMAGENET1K_V2",
        )
    distillation_training.train()
```
1. imagenet_dir 是一个指向图像数据集的目录路径
2. channel_size 是一个整数变量
3. save_path 是一个保存路径
4. 接下来，代码检查 save_path 是否存在，如果不存在，则创建该路径。
5. 然后，创建了一个 DistillationTraining 对象，并传递了一些参数给它。
    ```python
        def __init__(self,imagenet_dir,channel_size,batch_size,save_path,normalize_iter,iteration=10000,resize=512,model_size='S', 
                wide_resnet_101_arch="Wide_ResNet101_2_Weights.IMAGENET1K_V2", print_freq=25,with_bn=False) -> None:
    ```
    * imagenet_dir：图像数据集的目录路径。
    * channel_size：通道大小，一个整数值。
    * batch_size：批处理大小，一个整数值。
    * save_path：保存路径，一个字符串。
    * normalize_iter：归一化迭代次数，一个整数值。
    * iteration：迭代次数，默认为10000。
    * resize：调整图像大小的目标尺寸，默认为512。
    * model_size：模型大小，一个字符串，默认为'S'。
    * wide_resnet_101_arch：Wide ResNet 101的架构，一个字符串，默认为"Wide_ResNet101_2_Weights.IMAGENET1K_V2"。
    * print_freq：打印频率，每训练多少次迭代打印一次信息，默认为25。
    * with_bn：是否使用批归一化，一个布尔值，默认为False。
6. 训练
    1. `self.load_pretrain()：`加载预训练模型
    2. `imagenet_dataset = ImageNetDataset(self.imagenet_dir, self.data_transforms)：`创建一个 ImageNetDataset 对象，用于加载图像数据集，并应用之前定义的图像转换操作
    3. `dataloader = DataLoader(imagenet_dataset, batch_size=self.batch_size, shuffle=True,num_workers=4, pin_memory=True)：`创建一个数据加载器，用于批量加载图像数据。它从 imagenet_dataset 中按照指定的 batch_size 进行加载，并进行随机打乱顺序。num_workers 参数指定了加载数据的线程数，pin_memory 参数用于将数据加载到固定的内存区域，以提高数据加载的效率
    4. `dataloader = load_infinite(dataloader)：`将数据加载器转换为无限循环的加载器，以便在训练过程中可以无限次地遍历数据集
    5. `teacher = Teacher(self.model_size)：`创建一个 Teacher 对象
    6. `teacher = teacher.cuda()：`将 teacher 模型移动到 GPU 上进行加速计算
    7. `self.mean,self.std = self.global_channel_normalize(dataloader)：`计算图像数据集的全局通道均值和标准差
    8. `optimizer = torch.optim.Adam(teacher.parameters(), lr=0.0001, weight_decay=0.00001)：`创建一个 Adam 优化器，用于优化教师模型的参数。学习率为 0.0001，权重衰减为 0.00001 (Adam 优化器:https://www.jiqizhixin.com/articles/2017-07-12)
    10. `scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(0.95 * self.train_iter), gamma=0.1)：`创建一个学习率调度器，用于调整优化器的学习率。它使用了 StepLR 调度器，每经过一定的训练迭代次数（step_size），学习率会按照指定的衰减因子（gamma）进行衰减
    11. `best_loss = 1000：`初始化最佳损失值为 1000。
    12. `loss_accum = 0：`初始化损失累积值为 0。
    13. `iteration = 0：`初始化迭代次数为 0。
    14. `for iteration in range(self.iteration):：`开始迭代训练过程。
        1. `batch_sample = next(dataloader).cuda()：`从数据加载器中获取一个批次的图像数据，并将其移动到 GPU 上。
        2. `teacher.train()：`将教师模型设置为训练模式。
        3. `optimizer.zero_grad()：`将优化器的梯度缓冲区清零。
        4. `loss = self.compute_mse_loss(teacher,batch_sample)：`计算教师模型的均方误差损失。
        5. `loss.backward()：`反向传播，计算梯度。
        6. `optimizer.step()：`更新模型的参数，执行优化步骤。
        7. `loss_accum += loss.item()：`累积损失值。
        8. `scheduler.step()：`更新学习率。
        9. `iteration+=1：`增加迭代次数。
        10. `(iteration+1) % self.print_freq == 0 and iteration > 100：`检查是否达到打印频率，并且迭代次数大于 100。
            1. `loss_mean = loss_accum/self.print_freq：`计算平均损失值。
            2. `print('iter:{},loss:{:.4f}'.format(iteration, loss_mean))：`打印当前迭代次数和平均损失值。
            3. `if loss_mean < best_loss or best_loss == 1000:：`检查当前损失是否是最佳损失，如果是，则保存最佳教师模型。
                1. `teacher.eval()：`将教师模型设置为评估模式。
                2. `torch.save(teacher.state_dict(), '{}/best_teacher.pth'.format(self.save_path))：`保存最佳教师模型的参数。
            4. `loss_accum = 0：`重置损失累积值。

        11. `teacher.eval()：`将教师模型设置为评估模式。
        12. `torch.save(teacher.state_dict(), '{}/last_teacher.pth'.format(self.save_path))：`保存最后一次迭代的教师模型的参数