[讲解原文](https://jalammar.github.io/illustrated-transformer/)
[中译摘抄原文](https://blog.csdn.net/qq_36667170/article/details/124359818)
### 从高层面看
我们先把整个Transformer模型看作是一个黑盒。在机器翻译中，它可以把句子从一种语言翻译成另一种语言。
![[transform1.png]]
打开这个黑盒，我们首先可以看到一个编码器（encoder）模块和一个解码器（decoder）模块，以及二者之间存在某种关联
![[transform2.png]]再往里看一下，编码器模块是6个encoder组件堆在一起，同样解码器模块也是6个decoder组件堆在一起。（为什么选6个呢？没有什么原因，论文原文就是这么写的，你也可以换成别的层数）
* 在这里这个encoder和decoder中的基础组件个数属于超参数,可以自己尝试设置不同的值。这是Transformer中第一个可调参数
![[transform3.png]]
6个编码器组件的结构是相同的（但是他们之间的权重是不共享的），每个编码器都可以分为2个子层。
![[transform4.png]]
编码器的输入首先会进入一个自注意力层，这个**注意力层的作用是：当要编码某个特定的词汇的时候，它会帮助编码器关注句子中的其他词汇**。之后会进行详细讲解。
自注意力层的输出会传递给一个前馈神经网络，每个编码器组件都是在相同的位置使用**结构相同**的前馈神经网络。
* 这里要注意一点,在前面我们提到6个组件之间的参数是不共享的。在这里虽然从自注意力层出来的结果进入相同的前馈神经网络。但是这里的相同仅限于结构相同,它们的参数是不同的。
解码器组件也含有前面编码器中提到的两个层，区别在于这两个层之间还夹了一个注意力层，多出来的这个自注意力层的作用是让解码器能够注意到输入句子中相关的部分
![[transform5.png]]
### 图解张量
**在一个已经训练好的Transformer模型中**，输入是怎么变为输出的呢？
首先我们要知道各种各样的张量或者向量是如何在这些组件之间变化的。

与其他的[[NLP]]项目一样，我们首先需要把输入的每个单词通过词嵌入（embedding）转化为对应的向量。
![[transform6.png]]
* 虽然这里只画了4个格子，但是我们用4个格子表示512的维度，不是说用4个格子表示四维。
所有编码器接收一组向量作为输入，论文中的输入向量的维度是512。最底下的那个编码器接收的是嵌入向量，之后的编码器接收的是前一个编码器的输出。
向量长度这个超参数是我们可以设置的，一般来说是我们训练集中最长的那个句子的长度。
当我们的输入序列经过词嵌入之后得到的向量会依次通过编码器组件中的两个层。
![[transform7.png]]
在这里，我们开始看到Transformer的一个关键属性，即每个位置上的单词在编码器中有各自的流通方向。在自注意力层中，这些路径之间存在依赖关系。 然而，前馈神经网络中没有这些依赖关系，因此各种路径可以在流过前馈神经网络层的时候并行计算。
#### eg
接下来，我们用一个短句（`Thinking Machine`）作为例子，看看在编码器的每个子层中发生了什么。

上边我们已经说了，每个编码器组件接受一组向量作为输入。在其内部，输入向量先通过一个自注意力层，再经过一个前馈神经网络，最后将其将输出给下一个编码器组件。
![[transform8.png]]
### 自注意力
在Transformer中，自注意力机制也可以将其他相关单词的“理解”融入到我们当前处理的单词中。
![[transform9.png]]
#### 细说自注意力机制
**第一步** 对编码器的每个输入向量都计算三个向量，就是对每个输入向量都算一个query、key、value向量。
把输入的词嵌入向量与三个权重矩阵相乘。权重矩阵是模型训练阶段训练出来的。
![[transform10.png]]
**注意**，这三个向量维度是64，比嵌入向量的维度小，嵌入向量、编码器的输入输出维度都是512。这三个向量**不是必须**比编码器输入输出的维数小，这样做主要是为了让多头注意力的计算更稳定。
![[transform11.png]]

> [!NOTE] 
>这三个向量是计算注意力时的抽象概念

**第二步** 计算注意力得分
